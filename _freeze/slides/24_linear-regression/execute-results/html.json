{
  "hash": "e096d2989763a7ff504d24322653469a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MATH167R: Linear regression\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## Overview of today\n\n-   Covariance and correlation\n-   Linear regression\n\n## Relating numerical variables\n\n![](images/Screenshot%202023-10-29%20at%2010.20.45%20AM.png){fig-alt=\"Scatter plot of survey data showing the percentage of 15- to 24-year-olds and 40+ year-olds who believe the world is getting better across various countries.\"}\n\n## Covariance and correlation\n\nIn statistics, correlation and covariance usually refer to a specific\ntype of association.\n\n-   If $X$ and $Y$ are linearly associated, then they are correlated.\n\n## Correlation\n\nCorrelation can also refer to the correlation coefficient.\n\nThe (Pearson) correlation coefficient $R$ is a statistic between $-1$\nand 1 that describes the strength and direction of the linear\nrelationship between two variables.\n\n$$R = \\frac{1}{n-1}\\sum_{i=1}^n\\frac{x_i-\\overline{x}}{s_x}\\frac{y_i-\\overline{y}}{s_y}$$\n\nThe sign of $R$ indicates the direction of the relationship. The\nabsolute value of $R$ indicates the strength.\n\n## Correlation and covariance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(mtcars)\ncor(mpg, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.8676594\n```\n\n\n:::\n\n```{.r .cell-code}\ncov(mpg, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -5.116685\n```\n\n\n:::\n:::\n\n\n\n## Pairs plots for assessing many relationships\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(~mpg + hp + wt, mtcars)\n```\n\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/unnamed-chunk-2-1.png){fig-alt='Matrix of scatter plots showing relationships between mpg, hp, and wt' width=960}\n:::\n:::\n\n\n\n## Regression methods\n\nAssume we have some data:\n\n-   $X_1,\\ldots, X_p$: $p$ **independent variables/explanatory\n    variables/covariates/predictors**\n-   $Y$: the **dependent variables/response/outcome**.\n\nWe want to know the relationship between our covariates and our\nresponse, we can do this with a method called **regression**. Regression\nprovides us with a statistical method to conduct inference and\nprediction.\n\n## Regression methods\n\n-   **inference:** assess the relationship between our variables, our\n    statistical model as a whole, predictor importance\n    -   What is the relationship between sleep and GPA?\n    -   Is parents' education or parents' income more important for\n        explaining income?\n-   **prediction:** predict new/future outcomes from new/future\n    covariates\n    -   Can we predict test scores based on hours spent studying?\n\n## Why is it called regression?\n\n![](https://upload.wikimedia.org/wikipedia/commons/b/b2/Galton%27s_correlation_diagram_1875.jpg){fig-alt=\"A scientific diagram showing the relationship between the heights of mid-parents and adult children, with axes labeled in inches and deviations, and an oval plot with labeled segments and data points.\"\nfig-align=\"center\"}\n\n## Simple linear regression\n\nHow does the speed of a car affect its stopping distance?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(dist ~ speed, data = cars,\n     xlab = \"Speed (mph)\",\n     ylab = \"Stopping Distance (ft)\",\n     pch  = 16, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/cars-1.png){fig-align='center' fig-alt='Scatter plot showing the relationship between speed (mph) and stopping distance (ft) with blue dots indicating data points.' width=60%}\n:::\n:::\n\n\n\n## Simple linear regression\n\nSuppose $x_1,\\ldots, x_n$ represent the `speed` values of each car in\nour dataset. Let $Y_1,\\ldots, Y_n$ represent the `dist` variable.\n\nThe **simple linear regression model** is given by\n$$Y_i=\\beta_0+\\beta_1x_i+\\epsilon_i$$ where:\n\n-   $\\beta_0$ is the intercept parameter\n\n-   $\\beta_1$ is the slope parameter\n\n-   $\\epsilon_i$ represents iid $N(0,\\sigma^2)$ errors, where $\\sigma^2$\n    is the variance of $\\epsilon_i$.\n\n## Simple linear regression\n\nUnder this model, we can also write that\n$$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$ As a result\n$$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n$$\\mathrm{Var}(Y_i\\mid X_i= x_i)= \\sigma^2$$\n\n## Simple linear regression\n\nTypically, we say there are four assumptions for simple linear\nregression:\n\n-   **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is\n    linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n-   **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent\n    of each other.\n-   **Normality**:\n    $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n-   **Equal variance**: The variance of the errors $\\epsilon_i$ is the\n    same for all values of $x_i$.\n\nNote that even if one or more of these assumptions breaks, linear\nregression can still be useful if used with caution.\n\n## Simple linear regression: Least squares\n\nTypically, we start with data $Y_1,\\ldots, Y_n$ and $x_1,\\ldots, x_n$.\nHow do we estimate $\\beta_0$ and $\\beta_1$?\n\nIn other words, what is the best line to use for modeling the\nrelationship between $X$ and $Y$?\n\nUsually, we use the least squares line, which is the solution to the\nfollowing optimization problem:\n$$\\arg\\min\\sum_{i=1}^n (y_i-(\\beta_0+\\beta_1x_i))^2=\\arg\\min\\sum_{i=1}^n (y_i-\\widehat{y}_i)^2$$\nwhere $\\widehat{y}_i$ is the model prediction for $y_i$.\n\n## Simple linear regression: the `lm` function\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_res = lm(dist ~ speed, data = cars)\nlm_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n```\n\n\n:::\n:::\n\n\n\n## Simple linear regression: the `lm` function\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(dist ~ speed, data = cars,\n     xlab = \"Speed (mph)\",\n     ylab = \"Stopping Distance (ft)\", \n     pch  = 16, col  = \"blue\")\nabline(lm_res,  col = \"red\")\n```\n\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/cars3-1.png){fig-align='center' fig-alt='Scatter plot showing the positive correlation between speed (mph) and stopping distance (ft) with blue data points and a red trend line.' width=60%}\n:::\n:::\n\n\n\n## Simple linear regression: Inference\n\nFor the simple linear regression model, typically we wish to carry out\ninference on $\\beta_1$. One way to do so is via a hypothesis test:\n\n$H_0:\\beta_1=0$ $H_a:\\beta_1\\not=0$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,\tAdjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n```\n\n\n:::\n:::\n\n\n\n## Simple linear regression: Inference\n\nWe can also construct a confidence interval for $\\beta_1$ if our focus\nisn't on developing evidence against any null hypothesis:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(lm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %    97.5 %\n(Intercept) -31.167850 -3.990340\nspeed         3.096964  4.767853\n```\n\n\n:::\n:::\n\n\n\n## Simple linear regression: Prediction\n\nPrediction generally involves estimating the value of new data $Y_{n+1}$\ngiven $X_{n+1}$. We can use the `predict()` function with our `lm()`\nresults:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(lm_res,\n        newdata = list(speed = 19.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n59.10288 \n```\n\n\n:::\n\n```{.r .cell-code}\n# get prediction interval\npredict(lm_res, \n        newdata = list(speed = 19.5),\n        interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 59.10288 27.68517 90.52059\n```\n\n\n:::\n:::\n\n\n\n## Simple linear regression: Prediction\n\nNote that the `predict()` function can also be used to generate\n\"confidence\" intervals:\n\n-   `interval = \"prediction\"`: Construct a confidence interval for\n    $Y_{n+1}|X_{n+1}=x_{n+1}$, the value of a new **observation** for\n    which $X_{n+1}=x_{n+1}$\n-   `interval = \"confidence\"`: Construct a confidence interval for\n    $E(Y_{n+1}|X_{n+1}=x_{n+1})$, the value of the **conditional mean**\n    when $X_{n+1}=x_{n+1}$\n\n## Simple linear regression: Prediction\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# prediction interval\npredict(lm_res, \n        newdata = list(speed = 19.5),\n        interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 59.10288 27.68517 90.52059\n```\n\n\n:::\n\n```{.r .cell-code}\n# confidence interval\npredict(lm_res, \n        newdata = list(speed = 19.5),\n        interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr     upr\n1 59.10288 53.54796 64.6578\n```\n\n\n:::\n:::\n\n\n\n## Warm-up\n\nWhat is the method of least squares and how does it relate to linear\nregression?\n\nClassify the following questions as **inference** or **prediction**:\n\n1.  Can we use high school students' SAT/ACT scores to estimate their\n    college GPAs?\n2.  Is there an association between political candidates' heights and\n    the number of votes they receive?\n3.  Do states with higher minimum wages have lower poverty rates?\n\n## Simple linear regression: Checking assumptions\n\nTypically, we say there are four assumptions for simple linear\nregression:\n\n-   **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is\n    linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n-   **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent\n    of each other.\n-   **Normality**:\n    $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n-   **Equal variance**: The variance of the errors $\\epsilon_i$ is the\n    same for all values of $x_i$.\n\n## Checking linearity\n\n::: columns\n::: {.column width=\"50%\"}\n**Linear**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/lin-1.png){fig-align='center' fig-alt='Scatter plot with data points and a red trend line showing a positive correlation between x and y axes.s' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Nonlinear**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/nonlin-1.png){fig-align='center' fig-alt='Scatter plot with black dots and a red trend line, showing a positive correlation.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Checking linearity with residual plot\n\nWe can also plot the residuals against our independent variable, where\nwe define the **residual** as follows:\n$$\\widehat{\\epsilon}_i = \\mathrm{Residual} = \\mathrm{Observed} - \\mathrm{Predicted}=y -\\widehat{y}$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/cars_resid-1.png){fig-align='center' fig-alt='Scatter plot showing residuals versus speed in mph, with a horizontal red line at zero.' width=60%}\n:::\n:::\n\n\n\n## Checking linearity with residual plot\n\n::: columns\n::: {.column width=\"50%\"}\n**Linear**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/lin_resid-1.png){fig-align='center' fig-alt='Scatter plot of residuals against x with a red horizontal line at y equals 0.' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Nonlinear**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/nonlin_resid-1.png){fig-align='center' fig-alt='Scatter plot of residuals versus variable x with a horizontal red reference line at y = 0.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Checking independence\n\nChecking independence is typically more difficult. We require that\nconditional on the dependent variables $x_1,\\ldots, x_n$, the response\nvariables $Y_1,\\ldots, Y_n$ are independent.\n\nExample of independence: survey data obtained via simple random sampling\n\nExample of dependence: height and weight measurements of penguins, where\neach penguin is measured twice\n\n## Checking normality of errors\n\nCan you spot the difference?\n\n::: columns\n::: {.column width=\"50%\"}\n**Normal errors**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/norm_err-1.png){fig-align='center' fig-alt='Scatter plot with black data points trending upward and a red regression line indicating a positive correlation between x (0 to 5) and y (0.0 to 12.5).' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Non-normal errors**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/nonnorm_err-1.png){fig-align='center' fig-alt='Scatter plot with black data points and a red trend line, x-axis labeled \\'x\\' and y-axis labeled \\'y\\'.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Checking normality of errors with quantile-quantile plot\n\nWe can use a quantile-quantile plot to see whether our estimated\nresiduals are approximately normally distributed.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_res <- lm(dist ~ speed, cars)\nqqnorm(resid(lm_res))\nqqline(resid(lm_res))\n```\n:::\n\n\n\n## Checking normality of errors with quantile-quantile plot\n\nWhat do you notice?\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/cars_resid_qq-1.png){fig-align='center' fig-alt='Normal Q-Q Plot with sample quantiles on the y-axis and theoretical quantiles on the x-axis. Points mostly follow the reference line from bottom-left to top-right.' width=60%}\n:::\n:::\n\n\n\n## Checking normality of errors with quantile-quantile plot\n\n::: columns\n::: {.column width=\"50%\"}\n**Normal errors**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/norm_err_qq-1.png){fig-align='center' fig-alt='Normal Q-Q plot comparing sample quantiles to theoretical quantiles with points clustering around a diagonal line. ' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Non-normal errors**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/nonnorm_err_qq-1.png){fig-align='center' fig-alt='Normal Q-Q plot with sample quantiles versus theoretical quantiles. Points do not fall on a line.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Checking equal variances (homoskedasticity)\n\nCan you spot the difference?\n\n::: columns\n::: {.column width=\"50%\"}\n**Equal variances**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/eq_var-1.png){fig-align='center' fig-alt='Scatter plot showing black dots with a red trend line indicating a positive correlation between x and y values.' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Unequal variances**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/uneq_var-1.png){fig-align='center' fig-alt='Scatter plot graph with labeled axes and a red line of best fit showing a positive linear relationship.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Checking equal variances (homoskedasticity) with residual plot\n\nCan you spot the difference using the residual plot?\n\n::: columns\n::: {.column width=\"50%\"}\n**Equal variances**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/eq_var_resid-1.png){fig-align='center' fig-alt='Scatter plot with black dots showing residuals across a range of x-values from 0 to 5, with a red horizontal line at y=0.' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Unequal variances**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/uneq_var_resid-1.png){fig-align='center' fig-alt='Scatter plot of residuals versus x with many black dots and a horizontal red line at residuals equal to 0. Points are not normally distributed around the line.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Multiple linear regression\n\nWhat if we have multiple input variables?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mtcars)\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n\n\n:::\n:::\n\n\n\n## Multiple linear regression\n\nGiven our response $Y$ and our predictors $X_1, \\ldots, X_p$, a **linear\nregression model** takes the form:\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n**Note:** If we wish to include a categorical covariate, we can add\nindicator variables for each category.\n\n## Multiple linear regression\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n-   $Y$: dependent variable, outcome, response\n-   $X_j$: independent variable, covariate, predictor\n-   $\\beta_0$: Intercept\n-   $\\beta_j$: coefficient, the expected difference in the response\n    between two observations differing by one unit in $X_j$, with all\n    other covariates identical.\n-   $\\epsilon$: error, noise, with mean $0$ and variance $\\sigma^2$\n\n## Multiple linear regression: example\n\nConsider the following example using the `penguins` data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n\n## Multiple linear regression: example\n\nSuppose we are interested in predicting `body_mass_g` using\n`flipper_length_mm`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/penguins2-1.png){fig-align='center' fig-alt='Scatter plot comparing flipper length in millimeters (x-axis) versus body mass in grams (y-axis) with a blue regression line showing a positive correlation' width=60%}\n:::\n:::\n\n\n\n## Multiple linear regression: example\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_res <- lm(body_mass_g ~ flipper_length_mm, data = penguins)\nsummary(lm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1058.80  -259.27   -26.88   247.33  1288.69 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -5780.831    305.815  -18.90   <2e-16 ***\nflipper_length_mm    49.686      1.518   32.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.3 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.759,\tAdjusted R-squared:  0.7583 \nF-statistic:  1071 on 1 and 340 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Multiple linear regression: example\n\nConsider the following residual plot, where we define the **residual**\nas follows:\n$$\\mathrm{Residual} = \\mathrm{Observed} - \\mathrm{Predicted}=y -\\widehat{y}$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/penguins_resid-1.png){fig-align='center' fig-alt='Scatter plot of residuals versus flipper length for three penguin species: Adelie (black dots), Chinstrap (red dots), and Gentoo (green dots).' width=100%}\n:::\n:::\n\n\n\n## Multiple linear regression: example\n\nWhat if I also want to include `species` in the model? There are a few\nways to do so.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/movies4-1.png){fig-align='center' fig-alt='Scatter plot showing penguin body mass versus flipper length for Adelie, Chinstrap, and Gentoo species.' width=100%}\n:::\n:::\n\n\n\n## Multiple linear regression: Group intercepts\n\nThe first way is to add `species` into the formula for our linear\nregression model:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_species_res <- lm(body_mass_g ~ flipper_length_mm + species,\n                     data = penguins)\nsummary(lm_species_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-927.70 -254.82  -23.92  241.16 1191.68 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -4031.477    584.151  -6.901 2.55e-11 ***\nflipper_length_mm    40.705      3.071  13.255  < 2e-16 ***\nspeciesChinstrap   -206.510     57.731  -3.577 0.000398 ***\nspeciesGentoo       266.810     95.264   2.801 0.005392 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 375.5 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7826,\tAdjusted R-squared:  0.7807 \nF-statistic: 405.7 on 3 and 338 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Multiple linear regression: Group intercepts\n\nIn this case, the model is as follows:\n$$Y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\epsilon_i$$ where:\n\n-   $Y$ is `body_mass_g`\n-   $x_1$ is `flipper_length_mm`\n-   $x_2$ is a **dummy variable** equal to 1 if the penguin is Chinstrap\n    and 0 otherwise.\n-   $x_3$ is a **dummy variable** equal to 1 if the penguin is Gentoo\n    and 0 otherwise.\n\nWhat happened to the Adelie penguins?\n\n## Multiple linear regression: Group intercepts\n\nWhen including a categorical variable in a linear regression model, R\nautomatically treats one category as the **reference group**.\n\nThe value of the intercept $\\beta_0$ thus represents the expected value\nof $Y$ when $x_1$ is zero for an Adelie penguin.\n\nHow do we interpret the other $\\beta$ parameters?\n\n## Multiple linear regression: Group intercepts\n\n-   $\\beta_1$ represents the expected change in $Y$ when flipper length\n    increases by 1 mm.\n-   $\\beta_2+\\beta_0$ represents the expected value of $Y$ when $x_1$ is\n    zero for a Chinstrap penguin.\n-   $\\beta_2+\\beta_0$ represents the expected value of $Y$ when $x_1$ is\n    zero for a Gentoo penguin.\n\n## Multiple linear regression: Group intercepts\n\nThis model thus yields three parallel lines, one for each group.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/penguins_species_2-1.png){fig-align='center' fig-alt='Scatter plot showing the relationship between flipper length and body mass for three penguin species: Adelie (red), Chinstrap (green), and Gentoo (blue). We have separate regression lines for each species.' width=60%}\n:::\n:::\n\n\n\n## Multiple linear regression: example\n\nNow we obtain the following residual plot:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/penguins_resid_2-1.png){fig-align='center' fig-alt='Scatter plot showing the difference between observed and predicted values against flipper length for Adelie (black dots), Chinstrap (red dots), and Gentoo (green dots) penguins. The residuals are now clearly centered around the horizontal y = 0 line.' width=100%}\n:::\n:::\n\n\n\n## Multiple linear regression: Group-specific slopes\n\nThe previous model gave us group-specific intercepts, resulting in\nparallel lines. What if we want to let the slope vary across each\nspecies?\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/penguins_slopes-1.png){fig-align='center' fig-alt='Scatter plot showing relationship between flipper length and body mass for Adelie, Chinstrap, and Gentoo penguins with respective trend lines.' width=100%}\n:::\n:::\n\n\n\n## Multiple linear regression: Group-specific slopes\n\nThe standard approach is to introduce interaction terms:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_int_res <- lm(body_mass_g ~ flipper_length_mm * species,\n                     data = penguins)\nsummary(lm_int_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-911.18 -251.93  -31.77  197.82 1144.81 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        -2535.837    879.468  -2.883  0.00419 ** \nflipper_length_mm                     32.832      4.627   7.095 7.69e-12 ***\nspeciesChinstrap                    -501.359   1523.459  -0.329  0.74229    \nspeciesGentoo                      -4251.444   1427.332  -2.979  0.00311 ** \nflipper_length_mm:speciesChinstrap     1.742      7.856   0.222  0.82467    \nflipper_length_mm:speciesGentoo       21.791      6.941   3.139  0.00184 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 370.6 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7896,\tAdjusted R-squared:  0.7864 \nF-statistic: 252.2 on 5 and 336 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Multiple linear regression: Group-specific slopes\n\nThe standard approach is to introduce interaction terms:\n\n$$Y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4(x_1 * x_2) + \\beta_5(x_1*x_3)+\\epsilon_i$$\nwhere:\n\n-   $\\beta_1$ is now the slope for our reference group (Adelie)\n-   $\\beta_1+\\beta_4$ is the slope for Chinstrap\n-   $\\beta_1+\\beta_5$ is the slope for Gentoo\n\n## Linear Regression\n\nWe can fully write out a linear regression model\n\n$$\n\\begin{equation}\n\\begin{bmatrix} y_1 \\\\ y_2\\\\ \\vdots \\\\ y_n \\end{bmatrix} = \n\\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k}\\\\\n1 & x_{2,1} & \\cdots & x_{2, k}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & \\cdots & x_{n, k}\\end{bmatrix}\n\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{k} \\end{bmatrix} +\n\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\n\\end{equation}\n$$\n\n## Matrix Notation\n\nThis can also be expressed in matrix form:\n\n$$\n\\begin{align}\n\\mathbf{Y} &= \\mathbf{X}\\beta + \\epsilon,\\\\\n\\epsilon &\\sim N(0,1)\n\\end{align}\n$$\n\n-   $\\mathbf{Y} \\in \\mathbb{R}^{n \\times 1}$: an n-dimensional vector of\n    the response\n-   $\\mathbf{X} \\in \\mathbb{R}^{n \\times (k+1)}$: a $((k+1)\\times n)$\n    matrix of the predictors (including intercept)\n-   $\\beta \\in \\mathbb{R}^{((k+1)\\times 1)}$: a $(k+1)$-dimensional\n    vector of regression parameters\n-   $\\epsilon \\in \\mathbb{R}^{n \\times 1}$: an n-dimensional vector of\n    the error term\n\n## $\\large \\epsilon$: Error term\n\n$\\epsilon$, pronounced epsilon, represents the **error term** of our\nmodel. We can model $Y$ as a linear function of the $X$'s, but in the\nreal world, the relationship won't always be perfect. There is noise! It\ncan come from\n\n-   Measurement error in the $X$'s\n-   Measurement error in the $Y$'s\n-   Unobserved/missing variables in the model\n-   Deviations in the true model from linearity\n-   True randomness\n\nIn linear regression, we assume that this error term is normally\ndistributed with mean zero and variance $\\sigma^2$.\n\n## $\\beta_0$: Intercept\n\n$\\beta_0$ is the **intercept term** of our model. Notice that\n\n$$\\mathbb{E}[Y|X_1 = X_2 = \\cdots = X_p = 0] = \\beta_0$$\n\nThus, $\\beta_0$ is the expected value of our response if all the\ncovariates are equal to $0$. This is also known as the y-intercept of\nour model.\n\n## $X_j$: Independent variable\n\n$X_j$ represents the $j$<sup>th</sup> independent variable in our model.\nNotice that\n$$\\mathbb{E}[Y|X_1,\\ldots, X_p] = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p$$\nWhat happens to this expectation if we increase $X_j$ by 1 unit, holding\neverything else constant?\n\nThe conditional expectation of $Y$ increases by $\\beta_j$.\n\n## $\\beta_j$: Coefficient\n\n$\\beta_j$ represents the $j$<sup>th</sup> regression coefficient in our\nmodel. From the previous slide, we saw that for every 1 unit increase in\n$X_j$, holding all other variables constant, the expected value of the\nresponse increases by $\\beta_j$. From this we can derive an\ninterpretation.\n\n**Interpretation of** $\\beta_j$: the expected difference in the response\nbetween two observations differing by one unit in $X_j$, with all other\ncovariates identical.\n\n## `lm()`: Linear Model\n\nWe fit a linear regression model in R using `lm()`. The first argument\nis a **formula**, which is a type of R object. Formulas typically take\nthe following form: `Y ~ X_1 + X_2 + ... + X_p`.\n\nThe dependent variable, `Y` goes on the left-hand side of the tilde `~`,\nwhich marks the formula. The independent variables are added on the\nright-hand side. Using this formula will give us a model in the form of\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## `lm()`: Linear Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mtcars)\nmy_lm <- lm(mpg ~ hp + wt, data = mtcars)\nclass(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"lm\"\n```\n\n\n:::\n\n```{.r .cell-code}\nmy_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nCoefficients:\n(Intercept)           hp           wt  \n   37.22727     -0.03177     -3.87783  \n```\n\n\n:::\n:::\n\n\n\n## `lm()`: Linear Model\n\nWe can see from `names()` that `lm` objects contain a lot more than they\nprint out by default.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n```\n\n\n:::\n:::\n\n\n\n## `summary()` Model summaries\n\n`summary()` or `summary.lm()` gives us a summary of our `lm` object in\nR.\n\n-   The quantiles of the residuals: hopefully, they match a normal\n    distribution.\n-   Coefficients, their standard errors, and their individual\n    significances\n-   (Adjusted) R-squared value: how much of the overall variability in\n    the response is explained by the model?\n-   F-statistic: hypothesis test for the significance of the overall\n    model\n\n## `summary()` Model summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n\n\n:::\n:::\n\n\n\n## `plot()`: Regression model diagnostics\n\nCalling `plot(my_lm)` will return several diagnostic plots. Remember\nthat we want our error term to look normally distributed with mean zero.\nWe won't go into all the details for this class, but here are some tips:\n\n-   **Residuals vs Fitted:** these are your errors (residuals) plotted\n    over the predicted outcome (fitted). Errors should be random, so\n    here you want to see randomly scattered points with no discernable\n    pattern. You want the trend line to be approximately horizontal.\n-   **Normal Q-Q plot:** These are the quantiles of your errors against\n    the quantiles of a normal distribution. In theory, your errors\n    should be normally distributed, so you are hoping that points are\n    mostly along the 45-degree $y=x$ line.\n\n## `plot()`: Regression model diagnostics\n\n-   **Scale-location:** This looks at the magnitude of standardized\n    residuals over the predicted outcome. Similar interpretation as\n    residuals vs fitted. This can make it slightly easier to identify\n    undesireable patterns.\n-   **Residuals vs leverage:** This can help identify highly influential\n    points, such as outliers. If points are outside dotted red lines,\n    then removing them would noticeably alter your results. *Never just\n    remove outliers!* If it's real data, it's valid and removing it will\n    bias your results. It is much more important to understand why\n    outliers are there than to remove them.\n\n## `plot()`: Regression model diagnostics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(my_lm)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/unnamed-chunk-9-1.png){fig-alt='Scatter plot titled Residuals vs Fitted showing fitted values on the x-axis and residuals on the y-axis with a red trend line and labeled data points.' width=960}\n:::\n:::\n\n\n\n## `plot()`: Regression model diagnostics\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/unnamed-chunk-10-1.png){fig-alt='Q-Q plot comparing theoretical quantiles to standardized residuals with some labeled data points.' width=960}\n:::\n:::\n\n\n\n## `plot()`: Regression model diagnostics\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/unnamed-chunk-11-1.png){fig-alt='Scatter plot of standardized residuals against fitted values with a red trend line and labeled points for Chrysler Imperial, Fiat 128, and Toyota Corolla.' width=960}\n:::\n:::\n\n\n\n## `plot()`: Regression model diagnostics\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/unnamed-chunk-12-1.png){fig-alt='Scatter plot titled \\'Residuals vs Leverage\\' with labeled data points for \\'Toyota Corolla,\\' \\'Chrysler Imperial,\\' and \\'Maserati Bora.\\' Plot shows standardized residuals on the vertical axis and leverage on the horizontal axis, with Cook\\'s distance thresholds indicated by dashed lines and a red regression line.' width=960}\n:::\n:::\n\n\n\n## `coef()`: Extract coefficients\n\nUse `coef()` to extract estimated coefficients as a vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          hp          wt \n37.22727012 -0.03177295 -3.87783074 \n```\n\n\n:::\n:::\n\n\n\n## `fitted()` Extract fitted values\n\nUse `fitted()` to extract the fitted/estimated values for the response.\nThis can be useful to compare how our fitted values compare to the\nestimated values to help assess model fit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nmod_fits <- fitted(my_lm)\nmy_df <- data.frame(actual = mtcars$mpg, fitted = mod_fits)\nggplot(my_df, aes(x = fitted, y = actual)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, col = \"red\", lty = 2) + \n  theme_bw(base_size = 15) +\n  labs(x = \"Fitted values\", y = \"Actual values\", title = \"Actual vs. Fitted\") +\n  theme(plot.title = element_text(hjust = 0.5))\n```\n:::\n\n\n\n## `fitted()` Extract fitted values\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n## `predict()`: Predict new outcomes\n\nUse `predict()` to predict new outcomes given new explanatory variables.\nFor example, pretend we observe two new cars with horsepowers of `100`\nand `150`, respectively, and weights of `3000` and `3500`, respectively.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note: wt is in 1000s of lbs\nnew_obs <- data.frame(hp = c(100, 150), wt = c(3, 3.5))\npredict(my_lm, new_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2 \n22.41648 18.88892 \n```\n\n\n:::\n:::\n\n\n\nWe'll come back to prediction in future lectures.\n\n## `residuals()`: Compute residuals\n\nUse `residuals()` to compute the residuals for fitted values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n        -2.57232940         -1.58348256         -2.47581872          0.13497989 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n         0.37273336         -2.37381631         -1.29904236          1.51293266 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n         0.80632669         -0.77945988         -2.17945988          0.67463146 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n         0.25616901         -1.64993945          0.04479541          1.03726743 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n         5.50751301          5.80097202          1.08761978          5.85379085 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n        -3.08644148         -3.31136386         -3.94097947         -1.25202805 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n         2.44325481         -0.32665313         -0.03737415          2.63023081 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n        -0.74648866         -1.22541324          2.26052287         -1.58364943 \n```\n\n\n:::\n:::\n\n\n\n## Manipulating formulas\n\nWorking with formulas in R can be somewhat confusing, so it is important\nto understand how formulas work to make sure you are fitting the\nintended model.\n\n## `- 1`\n\nUse `- 1` to remove an intercept from your model. Only do this if you\nare very sure that what you are doing is appropriate. I don't recommend\ndoing this in practice.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_intercept <- lm(mpg ~ hp + wt - 1, data = mtcars)\nsummary(no_intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp + wt - 1, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.407  -2.382   2.511   7.091  23.885 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)   \nhp -0.03394    0.03940  -0.861   0.3959   \nwt  6.84045    1.89425   3.611   0.0011 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.32 on 30 degrees of freedom\nMultiple R-squared:  0.7264,\tAdjusted R-squared:  0.7082 \nF-statistic: 39.83 on 2 and 30 DF,  p-value: 3.599e-09\n```\n\n\n:::\n:::\n\n\n\n## `:` operator\n\nUse `X1:X2` to include an interaction effect in your model. This is\nuseful if you have reason to believe two covariates interact, such as\ngender and education in a wage model. In our case, we'll assume\nhorsepower and weight interact in their effect on mpg.\n\nTypically (always?), if you include an interaction effect, you should\nalso include the marginal effects. You can do this automatically using\n`X1*X2`.\n\n## `:` operator\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteract <- lm(mpg ~ hp:wt, data = mtcars)\nsummary(interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp:wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8831 -2.0952 -0.4577  1.2262  7.9282 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 27.745642   1.062236   26.12  < 2e-16 ***\nhp:wt       -0.014872   0.001727   -8.61 1.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.288 on 30 degrees of freedom\nMultiple R-squared:  0.7119,\tAdjusted R-squared:  0.7023 \nF-statistic: 74.14 on 1 and 30 DF,  p-value: 1.321e-09\n```\n\n\n:::\n:::\n\n\n\n## `*` operator\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteract <- lm(mpg ~ hp*wt, data = mtcars)\nsummary(interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp:wt        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,\tAdjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13\n```\n\n\n:::\n:::\n\n\n\n## `.` operator\n\nUse `~ .` to include all non-response variables in the input data as\nindependent variables.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869,\tAdjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n```\n\n\n:::\n:::\n\n\n\n## Estimation\n\nHow do we choose/estimate $\\beta_{(k+1)\\times1}$?\n\nLeast squares finds the line that minimizes the squared distance between\nthe points and the line, i.e. makes\n$$\\left[y_i - (\\beta_0 + \\beta_1 x_{i, 1} + \\dots + \\beta_k x_{i,k})\\right]^2$$\nas small as possible for all $i = 1, \\dots, n$.\n\nThe vector $\\widehat{\\beta}$ that minimizes the sum of the squared\ndistances is ()\n\n$$ \\widehat{\\beta}=\\left(\\mathbf{X}^T \\mathbf{X} \\right)^{-1}\\mathbf{X}^T \\mathbf{Y}.$$\n\nNote: In statistics, once we have estimated a parameter we put a \"hat\"\non it, e.g. $\\widehat{\\beta_0}$ is the estimate of the true parameter\n$\\beta_0$.\n\n## Overview of today\n\n-   Review of linear regression\n-   Why use a linear model?\n-   Pitfalls of linear regression: breaking assumptions and more\n\n## Why linear regression?\n\nRegression methods focus on modeling the relationship between response\n$Y$ and explanatory variables $X_1,\\ldots, X_p$.\n\nLinear regression proposes a model of the form\n$$Y=\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p+\\epsilon; \\hspace{1em} \\epsilon\\sim N(0, \\sigma^2)$$\nSometimes, the following weaker form is used (why is this weaker?)\n$$E(Y\\mid X_1,\\ldots, X_p)=\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p$$ In\nother words, the expected value of $Y$ (given $X_1,\\ldots, X_p$) is a\nlinear transformation of $X_1,\\ldots, X_p$.\n\n## Why linear regression?\n\n1.  **Simplicity**: Easy to fit, usually gets the general trend correct,\n    hard to **overfit**\n2.  **Interpretability**: Consider the following log-normal model:\n    $$Y=\\exp(\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p+\\epsilon); \\hspace{1em} \\epsilon\\sim N(0, \\sigma^2)$$\n    How do we interpret $\\beta_1$?\n\n## Additivity\n\nNote that linear regression can often be generalized to encompass other\nmodels. Consider the following model:\n$$Y=\\beta_0+\\beta_1X_1 +\\beta_2X_1^2$$ This is an example of polynomial\nregression, but the methods for fitting this model are essentially the\nsame as ordinary least squares, using $X_1^2$ as an extra covariate.\n\nMany alternative models can be written in this form by transforming the\ncovariates or transforming the response $Y$.\n\n## Assumptions of linear regression\n\n-   **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is\n    linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n-   **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent\n    of each other.\n-   **Normality**:\n    $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n-   **Equal variance**: The variance of the errors $\\epsilon_i$ is the\n    same for all values of $x_i$.\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"60%\"}\n::: smaller\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/lm_ex-1.png){fig-align='center' fig-alt='Scatter plot with an upward-sloping red trend line, data points dispersed roughly linearly, x-axis labeled x and y-axis labeled y' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"45%\"}\n::: smaller\n**Inference**: Asymptotically valid confidence intervals and hypothesis\ntests for $\\beta_0$ and $\\beta_1$.\n:::\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5577 -0.6187 -0.0789  0.6198  3.1659 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.8980     0.3527   2.546   0.0151 *  \nx             2.0373     0.1170  17.420   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.066 on 38 degrees of freedom\nMultiple R-squared:  0.8887,\tAdjusted R-squared:  0.8858 \nF-statistic: 303.4 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"45%\"}\n::: smaller\n**Prediction**: Assuming the model holds for new data, unbiased point\npredictions and asymptotically valid prediction intervals for $Y_{n+1}$.\n:::\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(lm_res, \n        newdata = list(x = 1), \n        interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit       lwr      upr\n1 2.935285 0.7165113 5.154058\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Breaking linearity\n\nWhat if $E(Y_i\\mid X_i= x_i)\\not= \\beta_0+\\beta_1x_i$?\n\n::: columns\n::: {.column width=\"60%\"}\n::: smaller\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x^3)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24_linear-regression_files/figure-revealjs/cubic_ex-1.png){fig-align='center' fig-alt='Scatter plot with black data points and a red diagonal line of best fit showing an increasing trend. The x-axis ranges from 0 to 5, and the y-axis from 0 to 200.' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Breaking linearity\n\nOur standard inference and prediction strategies no longer work in\ngeneral.\n\n**Inference**: Confidence intervals and hypothesis tests for $\\beta_0$\nand $\\beta_1$ are no longer valid.\n\n**Prediction**: Point predictions are no longer unbiased and prediction\nintervals are no longer valid.\n\n## Collinearity\n\nWhat if our predictor variables are closely related? Consider the\nfollowing simulation:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50\nx1 <- rnorm(n)\nx2 <- x1 * 2\ne <- rnorm(n)\ny <- 1 + 3 * x1 + e\nlm(y ~ x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1)\n\nCoefficients:\n(Intercept)           x1  \n      1.156        2.814  \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(y ~ x1 + x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nCoefficients:\n(Intercept)           x1           x2  \n      1.156        2.814           NA  \n```\n\n\n:::\n:::\n\n\n\n## Collinearity\n\nWhat if our predictor variables are closely related? Consider the\nfollowing simulation:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx2b <- x1 * 2 + rnorm(n, sd = .01)\nlm(y ~ x1 + x2b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2b)\n\nCoefficients:\n(Intercept)           x1          x2b  \n      1.156       -2.759        2.787  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}