{
  "hash": "35da9f8f53a1f455dfd8950343ea7828",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MATH167R: Hypothesis Testing\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: visual\n---\n\n\n\n## Overview of today\n\n-   Simulations for power analysis\n-   P-hacking simulations\n\n## Permutation tests\n\nTypically, for hypothesis testing, we need to know the sampling distribution of the test statistic when the null hypothesis is true.\n\nIn some cases, we can derive the null sampling distribution analytically.\n\n**What if we don't know the sampling distribution under the null?** A permutation test is simple way to estimate the sampling distribution for any test statistic, requiring only some exchangeability assumptions on the data.\n\n## Permutation tests\n\n**Example:** Suppose we want to understand whether carrying a particular genetic variant affects an individual's height $y$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarrier <- rep(c(0,1), c(100,200))\n\n# an example where y is independent of the gene\nnull_y <- rnorm(300) \n# an example where y is dependent on the gene\nalt_y <- rnorm(300, mean = carrier * 5) \n```\n:::\n\n\n\n## Permutation tests\n\nIf the null hypothesis is true, the distribution of $Y$ for the carriers should look the same as the distribution for the non-carriers. If we **permute** the labels repeatedly, we can get resampled datasets.\n\nIf the null hypothesis is true, the shuffled data sets will resemble the original dataset. If the null hypothesis is false, the shuffled dataset may not look like the real data.\n\n## Null hypothesis true\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/unnamed-chunk-2-1.png){fig-alt='Side-by-side comparison of three heatmaps labeled \\'Data\\', \\'Shuffled\\', and \\'Shuffled and reordered\\' showing different distributions of red and blue shading.' width=960}\n:::\n:::\n\n\n\n## Null hypothesis false\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/unnamed-chunk-3-1.png){fig-alt='Three heatmaps showing data arrangements: original, shuffled, and shuffled and reordered' width=960}\n:::\n:::\n\n\n\n## Permutation tests\n\n1.  Calculate a test statistic based on the observed data.\n2.  Repeatedly permute the group labels to create resamples. For each resample, compute the resample test statistic.\n3.  Compare the observed data test statistic with the distribution of resampled test statistics.\n\n## Permutation tests\n\nIn the case of our simulated data, we know the true distribution of the difference of sample means. We can thus use a $t$-test to perform our hypothesis test:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(null_y[carrier == 0], null_y[carrier == 1], var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  null_y[carrier == 0] and null_y[carrier == 1]\nt = 0.40101, df = 298, p-value = 0.6887\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1957495  0.2959423\nsample estimates:\n  mean of x   mean of y \n-0.03127240 -0.08136878 \n```\n\n\n:::\n:::\n\n\n\n## Permutation tests\n\nCompare with the $t$-test for the alternative hypothesis data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(alt_y[carrier == 0], alt_y[carrier == 1], var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  alt_y[carrier == 0] and alt_y[carrier == 1]\nt = -41.436, df = 298, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.108675 -4.645416\nsample estimates:\nmean of x mean of y \n0.1121617 4.9892071 \n```\n\n\n:::\n:::\n\n\n\n## Permutation tests\n\nFor now, though, let's pretend we don't know the true null sampling distribution of our test statistic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nnull_diff <- mean(null_y[carrier==1]) - mean(null_y[carrier==0])\nsingle_test <- function(label, y) {\n  resample <- sample(label)\n  # resample test statistic\n  mean(y[resample == 1]) - mean(y[resample == 0])\n}\ntest_stats_null <- replicate(1000, single_test(carrier, null_y))\n```\n:::\n\n\n\n## Permutation tests\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(test_stats_null)\nabline(v = null_diff, lwd=2, col=\"purple\")\n```\n\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/unnamed-chunk-7-1.png){fig-alt='Histogram of test_stats_alt with a bell-shaped distribution and a vertical purple line slightly above zero.' width=960}\n:::\n\n```{.r .cell-code}\nmean(abs(test_stats_null) > abs(null_diff)) # P-value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.691\n```\n\n\n:::\n:::\n\n\n\n## Permutation tests\n\nCompare with the case where the null hypothesis is false.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nalt_diff <- mean(alt_y[carrier==1]) - mean(alt_y[carrier==0])\ntest_stats_alt <- replicate(1000, single_test(carrier, alt_y))\nhist(test_stats_alt, xlim = c(-0.5, 6))\nabline(v = alt_diff, lwd=2, col=\"purple\")\n```\n\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/unnamed-chunk-8-1.png){fig-alt='Histogram of test_stats_alt with a bell-shaped distribution and a vertical purple line slightly above zero.' width=960}\n:::\n\n```{.r .cell-code}\nmean(abs(test_stats_alt) > abs(alt_diff)) # P-value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n## Permutation tests: `sleep` data\n\nWhat if we apply this idea to the `sleep` data?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  extra group ID\n1   0.7     1  1\n2  -1.6     1  2\n3  -0.2     1  3\n4  -1.2     1  4\n5  -0.1     1  5\n6   3.4     1  6\n```\n\n\n:::\n:::\n\n\n\n## Permutation tests: `sleep` data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_stat <- mean(sleep$extra[sleep$group == 1]) - \n  mean(sleep$extra[sleep$group == 2])\nsingle_test <- function(label, y) {\n  resample <- sample(label)\n  # resample test statistic\n  mean(y[resample == 1]) - mean(y[resample == 2])\n}\ntest_stats_alt <- replicate(1000, single_test(sleep$group, sleep$extra))\nhist(test_stats_alt)\nabline(v = test_stat, lwd=2, col=\"purple\")\n```\n\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/unnamed-chunk-10-1.png){fig-alt='Histogram of test_stats_alt with a purple vertical line at approximately -2.' width=960}\n:::\n\n```{.r .cell-code}\nmean(abs(test_stats_alt) > abs(test_stat)) # P-value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.099\n```\n\n\n:::\n:::\n\n\n\n## Power analysis\n\nRecall that the power of a hypothesis test is the probability of rejecting the null hypothesis when the null hypothesis is false.\n\nNote the power will depend on the size of the difference between the true sampling distribution and the null hypothesis sampling distribution.\n\n## One sample $z$-test\n\nSuppose that $X_1,\\ldots, X_n\\sim N(\\mu, 1)$.\n\nConsider the hypotheses $H_0:\\mu=0$ and $H_a:\\mu\\neq 0$.\n\nWe reject at the $\\alpha$ significance level if $$|\\overline{X}_n| > \\frac{z_{\\alpha/2}}{\\sqrt{n}}$$\n\nIf $\\mu=\\mu_a\\neq 0$, then $$\\overline{X}_n\\sim N\\left(\\mu_a, \\frac{1}{\\sqrt{n}}\\right)$$\n\n## One sample $z$-test\n\n$$\n\\begin{align}\nP\\left(|\\overline{X}_n| > \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\right)&=P\\left(\\overline{X}_n> \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\right) + P\\left(\\overline{X}_n < -\\frac{z_{\\alpha/2}}{\\sqrt{n}}\\right)\\\\\n&=P\\left(\\frac{\\overline{X}_n-\\mu_a}{1/\\sqrt{n}}> z_{\\alpha/2}-\\frac{\\mu_a}{1/\\sqrt{n}}\\right) \\\\\n&+ P\\left(\\frac{\\overline{X}_n-\\mu_a}{1/\\sqrt{n}}<-z_{\\alpha/2}-\\frac{\\mu_a}{1/\\sqrt{n}}\\right)\\\\\n&=\\left(1-\\Phi\\left(z_{\\alpha/2}-\\frac{\\mu_a}{1/\\sqrt{n}}\\right)\\right)+\\Phi\\left(-z_{\\alpha/2}-\\frac{\\mu_a}{1/\\sqrt{n}}\\right)\n\\end{align}\n$$\n\n## One sample $z$-test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_sample_z_power <- function(mu_a, alpha, n) {\n  z <- qnorm(1 - alpha / 2)\n  return(1 - pnorm(z - mu_a * sqrt(n)) + pnorm(-z - mu_a * sqrt(n)))\n}\n\n# plot the power function\nx <- data.frame(x = seq(-3, 3, length.out = 1000))\nggplot(x, aes(x = x)) + \n  geom_function(fun = one_sample_z_power, \n                aes(color = \"n = 5\"),\n                args = list(alpha = .05, n = 5)) +\n  geom_function(fun = one_sample_z_power, \n                aes(color = \"n = 10\"),\n                args = list(alpha = .05, n = 10)) +\n  geom_function(fun = one_sample_z_power, \n                aes(color = \"n = 20\"),\n                args = list(alpha = .05, n = 20)) +\n  xlab(\"True population mean\") + ylab(\"Power\") +\n  theme_bw()\n```\n:::\n\n\n\n## One sample $z$-test\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/unnamed-chunk-12-1.png){fig-alt='Line graph showing power versus true population mean for three sample sizes: n=10 (red), n=20 (green), and n=5 (blue).' width=960}\n:::\n:::\n\n\n\n## Using simulations to estimate power\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_test <- function(mu_a, alpha, n) {\n  Xbar <- mean(rnorm(n, mean = mu_a, sd = 1))\n  Z_score <- Xbar * sqrt(n)\n  return(abs(Z_score) > qnorm(1 - alpha / 2))\n}\npower_sim <- function(mu_a, alpha, n) {\n  return(mean(replicate(1000, simulate_test(mu_a, alpha, n))))\n}\n\nmu_a <- data.frame(mu_a = seq(-2.5, 2.5, length.out = 100))\nsample_sizes <- c(5, 10, 20)\n\n# matrix of simulation results\nsim_results <- \n  sapply(sample_sizes,\n         function(n) \n           sapply(mu_a$mu_a, \n                  function(mu)\n                    power_sim(mu, .05, n)))\n\nggplot(mu_a, aes(x = mu_a)) +\n  geom_line(aes(y = sim_results[, 1], color = \"n = 5\")) +\n  geom_line(aes(y = sim_results[, 2], color = \"n = 10\")) +\n  geom_line(aes(y = sim_results[, 3], color = \"n = 20\")) +\n  xlab(\"True population mean\") + ylab(\"Power\") +\n  theme_bw()\n```\n:::\n\n\n\n## Using simulations to estimate power\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/power.sim.eval-1.png){fig-alt='Line graph showing power vs. true population mean with curves for n = 5, n = 10, and n = 20.' width=960}\n:::\n:::\n\n\n\n## Exercise: estimating power\n\nIf $X_1,\\ldots X_n\\sim N(1, 1)$, what is the power for a one-sample $t$ test for the hypotheses $H_0:\\mu = 0$ and $H_a:\\mu\\not=0$ if $n=30$?\n\nCreate a simulation to estimate the power.\n\n## P-hacking simulations\n\n**P-hacking** refers to the practice of repeatedly performing hypothesis tests (and potentially manipulating the data) until a statistically significant P-values is obtained. Usually, only this final result is published, without mentioning all of the manipulations that came before.\n\n## P-hacking simulations\n\nSuppose we simulate 25 observations of 8 variables which we know to be uncorrelated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_signal_data <- matrix(rnorm(200), ncol = 8) # 25 x 8 matrix\npairs(no_signal_data) # pairwise scatter plots\n```\n\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/phack.sim-1.png){fig-alt='Matrix of 64 scatter plots arranged in an 8x8 grid, with rows and columns labeled var 1 to var 8' width=960}\n:::\n:::\n\n\n\n## P-hacking simulations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_signal_data <- matrix(rnorm(200), ncol = 8) # 25 x 8 matrix\npairs(no_signal_data) # pairwise scatter plots\n```\n\n::: {.cell-output-display}\n![](23_hypothesis-testing-III_files/figure-revealjs/phack.sim.eval-1.png){fig-alt='Matrix of 64 scatter plots arranged in an 8x8 grid, with rows and columns labeled var 1 to var 8' width=960}\n:::\n:::\n\n\n\n## Multiple testing simulations\n\nWhat if we perform a hypothesis test to test whether the correlation is zero between each pair of variables using `cor.test()`? With only 8 variables, we have 28 potential comparisons, the probability that we will (falsely) reject the null is already:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - (0.95) ^ 28\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7621731\n```\n\n\n:::\n:::\n\n\n\n## Multiple testing simulations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1029)\nno_signal_data<- matrix(rnorm(200), ncol = 8) # 25 x 8 matrix\npairs_to_compare <- combn(8, 2) # all combinations of 2 numbers from 1-8\np_values <- c()\nfor (i in 1:ncol(pairs_to_compare)) {\n  index_1 <- pairs_to_compare[1, i]\n  index_2 <- pairs_to_compare[2, i]\n  test_res <- \n    cor.test(no_signal_data[, index_1],\n             no_signal_data[, index_2])\n  p_values <- c(p_values, test_res$p.value)\n}\nprint(min(p_values))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006656272\n```\n\n\n:::\n:::\n\n\n\n## Multiple testing simulations\n\nFor this reason it is common to perform a correction to the p-values when many hypothesis tests are conducted.\n\n**Example**: The Bonferroni correction divides $\\alpha$ by the number of tests performed to get the corrected significance level.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}